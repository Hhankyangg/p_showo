{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from models import Showo, MAGVITv2, get_mask_chedule\n",
    "from training.prompting_utils import UniversalPrompting, create_attention_mask_predict_next, create_attention_mask_for_mmu\n",
    "from training.utils import get_config, flatten_omega_conf, mask_or_random_replace_tokens, AverageMeter\n",
    "from transformers import AutoTokenizer\n",
    "from models.clip_encoder import CLIPVisionTower\n",
    "from transformers import CLIPImageProcessor\n",
    "from llava.llava import conversation as conversation_lib\n",
    "\n",
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"phi1.5\"]\n",
    "\n",
    "import os\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "config = OmegaConf.load('configs/showo_demo.yaml')\n",
    "# device setup\n",
    "device = torch.device(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 13, 16, 16) = 3328 dimensions.\n",
      "Look-up free quantizer with codebook size: 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'mask_token_id': 58497} were passed to Showo, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/home/hpyky/Show-o/models/modeling_showo.py:49: FutureWarning: Accessing config attribute `w_clip_vit` directly via 'Showo' object attribute is deprecated. Please access 'w_clip_vit' over 'Showo's config object instead, e.g. 'unet.config.w_clip_vit'.\n",
      "  if self.w_clip_vit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention implementation:  sdpa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Showo(\n",
       "  (showo): PhiForCausalLM(\n",
       "    (model): PhiModel(\n",
       "      (embed_tokens): Embedding(58498, 2048)\n",
       "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x PhiDecoderLayer(\n",
       "          (self_attn): PhiSdpaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (k_layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (rotary_emb): PhiRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): PhiMLP(\n",
       "            (activation_fn): NewGELUActivation()\n",
       "            (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=58498, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.showo.llm_model_path, padding_side =\"left\")\n",
    "uni_prompting = UniversalPrompting(tokenizer, max_text_len=config.dataset.preprocessing.max_seq_length,\n",
    "                                       special_tokens=(\"<|soi|>\", \"<|eoi|>\", \"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n",
    "                                       ignore_id=-100, cond_dropout_prob=config.training.cond_dropout_prob)\n",
    "vq_model = MAGVITv2\n",
    "vq_model = vq_model.from_pretrained(config.model.vq_model.vq_model_name).to(device)\n",
    "vq_model.requires_grad_(False)\n",
    "vq_model.eval()\n",
    "model = Showo.from_pretrained(config.model.showo.pretrained_model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 1  # retain only the top_k most likely tokens, clamp others to have 0 probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新 token ID: [50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321]\n",
      "新增文本 token ID: [50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321]\n",
      "Concept Token '<dunpai>' 的新 ID: 50305\n",
      "嵌入层大小: torch.Size([58515, 2048])\n",
      "index_no_updates 中 False 的位置: tensor([50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314,\n",
      "        50315, 50316, 50317, 50318, 50319, 50320, 50321])\n",
      "index_no_updates 中 True 的数量: tensor(58498)\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/home/hpyky/full_mcdata\"\n",
    "concept = \"dunpai\"\n",
    "ckpt_name = \"t2i_wo_sys\"\n",
    "epoch2load = 90\n",
    "\n",
    "ckpt_path = os.path.join(\"saves\", concept, ckpt_name)\n",
    "ckpt_embed_path = os.path.join(ckpt_path, f\"epoch_{epoch2load}_embed.pt\")\n",
    "ckpt_lm_head_weight_path = os.path.join(ckpt_path, f\"epoch_{epoch2load}_lm_head_weight.pt\")\n",
    "ckpt_lm_head_bias_path = os.path.join(ckpt_path, f\"epoch_{epoch2load}_lm_head_bias.pt\")\n",
    "\n",
    "nums_new_token_i = 16\n",
    "\n",
    "#################################\n",
    "new_tokens = [f\"<{concept}>\"] + [f\"<token_{i}>\" for i in range(nums_new_token_i)]\n",
    "num_new_tokens = len(new_tokens)  # 17\n",
    "\n",
    "# 已知的原始参数\n",
    "# 文本 token 数量（ID 0-50304）\n",
    "original_text_vocab_size = len(tokenizer)  \n",
    "# Image token 数量（原 ID 50305-58497）\n",
    "original_image_vocab_size = model.showo.get_input_embeddings().num_embeddings - len(tokenizer)\n",
    "\n",
    "original_total_vocab = original_text_vocab_size + original_image_vocab_size  # 58498\n",
    "\n",
    "# 新的参数\n",
    "new_text_vocab_size = original_text_vocab_size + num_new_tokens  # 50305 + 17 = 50322\n",
    "new_total_vocab = original_total_vocab + num_new_tokens          # 58498 + 17 = 58515\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: 修改 Tokenizer 的词汇表\n",
    "# ------------------------------\n",
    "\n",
    "# 添加新 token 到 50305-50321 的位置\n",
    "num_new_tokens = tokenizer.add_tokens(new_tokens)\n",
    "new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "print(\"新 token ID:\", new_token_ids)  # 应输出 50305-50321\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: 调整模型的权重\n",
    "# ------------------------------\n",
    "with torch.no_grad():\n",
    "    # 获取嵌入层权重\n",
    "    embeddings = model.showo.get_input_embeddings().weight.data\n",
    "    \n",
    "    # 扩展嵌入层（58498 -> 58515）\n",
    "    model.showo.resize_token_embeddings(new_total_vocab)\n",
    "    # new_embeddings = model.showo.get_input_embeddings().weight.data\n",
    "\n",
    "    # 将原 Image Token 权重后移 17 位\n",
    "    original_image_weights = embeddings[original_text_vocab_size:original_total_vocab].clone()\n",
    "    model.showo.get_input_embeddings().weight.data[new_text_vocab_size:new_total_vocab] = original_image_weights\n",
    "    \n",
    "    # 初始化新 token 的权重（用原文本最后 17 个 token）\n",
    "    if os.path.exists(ckpt_embed_path):\n",
    "        ckpt_embed_weight = torch.load(ckpt_embed_path)\n",
    "        with torch.no_grad():\n",
    "            model.showo.get_input_embeddings().weight.data[original_text_vocab_size:new_text_vocab_size] = ckpt_embed_weight.to(model.showo.get_input_embeddings().weight.device)\n",
    "    else:\n",
    "        raise ValueError(\"Embedding weights do not exist!\")\n",
    "        \n",
    "    # new_text_weights = embeddings[original_text_vocab_size - num_new_tokens : original_text_vocab_size].clone()\n",
    "    # model.showo.get_input_embeddings().weight.data[original_text_vocab_size : new_text_vocab_size] = new_text_weights\n",
    "    # print(model.showo.lm_head.weight.data.shape[1])\n",
    "    # 处理 lm_head（假设与嵌入层共享权重）\n",
    "    if model.showo.lm_head.weight.data.shape[0] == new_total_vocab:\n",
    "        # 扩展 lm_head 权重\n",
    "        lm_head = model.showo.lm_head\n",
    "        new_lm_head = torch.nn.Linear(\n",
    "            lm_head.in_features, \n",
    "            new_total_vocab, \n",
    "            bias=hasattr(lm_head, 'bias')\n",
    "        )\n",
    "        new_lm_head.weight.data = lm_head.weight.data.clone()\n",
    "        new_lm_head.weight.data[new_text_vocab_size:new_total_vocab] = lm_head.weight.data[original_text_vocab_size:original_total_vocab]\n",
    "        \n",
    "        if os.path.exists(ckpt_lm_head_weight_path):\n",
    "            ckpt_lm_head_weight = torch.load(ckpt_lm_head_weight_path)\n",
    "            with torch.no_grad():\n",
    "                new_lm_head.weight.data[original_text_vocab_size:new_text_vocab_size] = ckpt_lm_head_weight.to(new_lm_head.weight.device)\n",
    "        else:\n",
    "            raise ValueError(\"lm_head weights do not exist!\")\n",
    "        # new_lm_head.weight.data[original_text_vocab_size:new_text_vocab_size] = lm_head.weight.data[original_text_vocab_size - num_new_tokens : original_text_vocab_size]\n",
    "        if hasattr(lm_head, 'bias'):\n",
    "            new_lm_head.bias.data = lm_head.bias.data.clone()\n",
    "            new_lm_head.bias.data[new_text_vocab_size:new_total_vocab] = lm_head.bias.data[original_text_vocab_size:original_total_vocab]\n",
    "            \n",
    "            if os.path.exists(ckpt_lm_head_bias_path):\n",
    "                ckpt_lm_head_bias = torch.load(ckpt_lm_head_bias_path)\n",
    "                with torch.no_grad():\n",
    "                    new_lm_head.bias.data[original_text_vocab_size:new_text_vocab_size] = ckpt_lm_head_bias.to(new_lm_head.weight.device)\n",
    "            else:\n",
    "                raise ValueError(\"lm_head bias do not exist!\")\n",
    "            # new_lm_head.bias.data[original_text_vocab_size:new_text_vocab_size] = lm_head.bias.data[original_text_vocab_size - num_new_tokens : original_text_vocab_size]\n",
    "        \n",
    "        model.showo.lm_head = new_lm_head\n",
    "    else:\n",
    "        raise ValueError(\"lm_head weights do not match the input embeddings!\")\n",
    "\n",
    "index_no_updates = torch.ones((new_total_vocab,), dtype=torch.bool)\n",
    "index_no_updates[new_token_ids] = False\n",
    "# ------------------------------\n",
    "# 验证\n",
    "# ------------------------------\n",
    "# 检查新 token 的 ID\n",
    "print(\"新增文本 token ID:\", [tokenizer.convert_tokens_to_ids(t) for t in new_tokens])  # 应输出 50305-50321\n",
    "\n",
    "# 检查一个原 Image Token 的新 ID\n",
    "sample_image_token = tokenizer.convert_ids_to_tokens(original_text_vocab_size)  # 原 ID 50305\n",
    "print(f\"Concept Token '{sample_image_token}' 的新 ID:\", tokenizer.convert_tokens_to_ids(sample_image_token))  # 应输出 50322\n",
    "\n",
    "# 检查嵌入层形状\n",
    "print(\"嵌入层大小:\", model.showo.get_input_embeddings().weight.shape)  # 应显示 torch.Size([58515, 2048])\n",
    "\n",
    "# 检查 index_no_updates 中 True 的位置和数量，True 应该是 new token ids\n",
    "print(\"index_no_updates 中 False 的位置:\", torch.nonzero(~index_no_updates).squeeze())  # 应输出 50305-50321\n",
    "print(\"index_no_updates 中 True 的数量:\", torch.sum(index_no_updates))  # 应输出 58498\n",
    "\n",
    "config.model.showo.llm_vocab_size = len(tokenizer) - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_personalized_prompt = f\"<{concept}> is \"\n",
    "for i in range(nums_new_token_i):\n",
    "    system_personalized_prompt += f\"<token_{i}>\"\n",
    "    if i == nums_new_token_i - 1:\n",
    "        system_personalized_prompt += \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 3, 256, 256])\n",
      "[' The color of the <dunpai> in the image is red, white, and blue.']\n"
     ]
    }
   ],
   "source": [
    "from pdata import image_transform\n",
    "from PIL import Image\n",
    "image_path = \"/home/hpyky/full_mcdata/two_concept/concept/test/dunpai/0.png\"\n",
    "question = system_personalized_prompt + \"\\n\" +'What is the color of the <dunpai> in the image?'\n",
    "# question = system_personalized_prompt + \"\\n\" + \"Can you see the <dunpai> in the image?\"\n",
    "## processing\n",
    "image_ori = Image.open(image_path).convert(\"RGB\")\n",
    "# tranforming the image to the required resolution:256x256\n",
    "image = image_transform(image_ori, resolution = config.dataset.params.resolution).to(device)\n",
    "image = image.unsqueeze(0)\n",
    "print(f\"image shape: {image.shape}\") # torch.Size([1, 3, 256, 256])\n",
    "\n",
    "image_tokens_mmu = vq_model.get_code(image)\n",
    "image_tokens = image_tokens_mmu + len(uni_prompting.text_tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "input_ids = uni_prompting.text_tokenizer(['USER: \\n' + question + ' ASSISTANT:'])[\n",
    "    'input_ids']\n",
    "input_ids = torch.tensor(input_ids).to(device)\n",
    "\n",
    "input_ids = torch.cat([\n",
    "    (torch.ones(input_ids.shape[0], 1) * uni_prompting.sptids_dict['<|mmu|>']).to(device),\n",
    "    (torch.ones(input_ids.shape[0], 1) * uni_prompting.sptids_dict['<|soi|>']).to(device),\n",
    "    image_tokens,\n",
    "    (torch.ones(input_ids.shape[0], 1) * uni_prompting.sptids_dict['<|eoi|>']).to(device),\n",
    "    (torch.ones(input_ids.shape[0], 1) * uni_prompting.sptids_dict['<|sot|>']).to(device),\n",
    "    input_ids\n",
    "], dim=1).long()\n",
    "\n",
    "attention_mask = create_attention_mask_for_mmu(input_ids.to(device),\n",
    "                                                eoi_id=int(uni_prompting.sptids_dict['<|eoi|>']))\n",
    "\n",
    "cont_toks_list = model.mmu_generate(input_ids, attention_mask=attention_mask,\n",
    "                            max_new_tokens=100, top_k=top_k,\n",
    "                            eot_token=uni_prompting.sptids_dict['<|eot|>'])\n",
    "\n",
    "cont_toks_list = torch.stack(cont_toks_list).squeeze()[None]\n",
    "\n",
    "text = uni_prompting.text_tokenizer.batch_decode(cont_toks_list, skip_special_tokens=True)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
