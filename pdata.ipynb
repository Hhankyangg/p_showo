{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/miniconda3/envs/showo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-03 20:53:06,135] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from models import Showo, MAGVITv2\n",
    "from training.prompting_utils import UniversalPrompting\n",
    "from transformers import AutoTokenizer\n",
    "from models.clip_encoder import CLIPVisionTower\n",
    "from transformers import CLIPImageProcessor\n",
    "from llava.llava import conversation as conversation_lib\n",
    "\n",
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"phi1.5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "config = OmegaConf.load('configs/showo_demo_w_clip_vit.yaml')\n",
    "# device setup\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 13, 16, 16) = 3328 dimensions.\n",
      "Look-up free quantizer with codebook size: 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'mask_token_id': 58497} were passed to Showo, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/home/arc/Show-o/models/modeling_showo.py:49: FutureWarning: Accessing config attribute `w_clip_vit` directly via 'Showo' object attribute is deprecated. Please access 'w_clip_vit' over 'Showo's config object instead, e.g. 'unet.config.w_clip_vit'.\n",
      "  if self.w_clip_vit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention implementation:  sdpa\n"
     ]
    }
   ],
   "source": [
    "# config load -  'showo_demo_w_clip_vit.yaml'\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "# show o tokenizer setup and adding special tokens to universal prompting\n",
    "# llm model : 'microsoft/phi-1_5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.showo.llm_model_path, padding_side =\"left\")\n",
    "uni_prompting = UniversalPrompting(tokenizer, max_text_len=config.dataset.preprocessing.max_seq_length,\n",
    "                                       special_tokens=(\"<|soi|>\", \"<|eoi|>\", \"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n",
    "                                       ignore_id=-100, cond_dropout_prob=config.training.cond_dropout_prob)\n",
    "\n",
    "# setting up the visual question answering model: magvit-v2\n",
    "vq_model = MAGVITv2\n",
    "vq_model = vq_model.from_pretrained(config.model.vq_model.vq_model_name).to(device)\n",
    "vq_model.requires_grad_(False)\n",
    "vq_model.eval()\n",
    "\n",
    "# setting up vision tower: clip-vit\n",
    "vision_tower_name =config.clip_path\n",
    "vision_tower = CLIPVisionTower(vision_tower_name).to(device)\n",
    "clip_image_processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n",
    "\n",
    "# setting up the showo model \n",
    "model = Showo.from_pretrained(config.model.showo.pretrained_model_path).to(device)\n",
    "\n",
    "# setting up the parameters\n",
    "temperature = 0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 1  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "SYSTEM_PROMPT = \"A chat between a curious user and an artificial intelligence assistant. \" \\\n",
    "                \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "SYSTEM_PROMPT_LEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalizedMMUDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        concept_name: str,\n",
    "        clip_image_processor,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.concept_name = concept_name\n",
    "        self.clip_image_processor = clip_image_processor\n",
    "\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[\"phi1.5\"]\n",
    "        with open(os.path.join(data_root, f\"training_data/{concept_name}.json\")) as f:\n",
    "            conversations = json.load(f)\n",
    "        self.conversations = conversations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conv_item = self.conversations[idx]\n",
    "        # {\n",
    "        #     \"messages\": [\n",
    "        #         {\n",
    "        #             \"content\": \"<image>How would you describe <BaGu>'s attire?\",\n",
    "        #             \"role\": \"user\"\n",
    "        #         },\n",
    "        #         {\n",
    "        #             \"content\": \"The image does not provide enough information to describe <BaGu>'s attire.\",\n",
    "        #             \"role\": \"assistant\"\n",
    "        #         }\n",
    "        #     ],\n",
    "        #     \"images\": [\n",
    "        #         \"/home/arc/MulBench/two_concept/concept/train/BaGu/6.png\"\n",
    "        #     ]\n",
    "        # }\n",
    "        image_path = conv_item[\"images\"][0]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = self.clip_image_processor(image, return_tensors=\"pt\")['pixel_values'][0]\n",
    "        \n",
    "        question = conv_item[\"messages\"][0][\"content\"].replace(\"<image>\", \"\")\n",
    "        answer = conv_item[\"messages\"][1][\"content\"]\n",
    "        \n",
    "        conv = conversation_lib.default_conversation.copy()\n",
    "        conv.append_message(conv.roles[0], question)\n",
    "        prompt_w_o_answer = conv.get_prompt()\n",
    "        conv.append_message(conv.roles[1], answer)\n",
    "        prompt_w_answer = conv.get_prompt()\n",
    "        \n",
    "        return {\n",
    "            # \"image\": image,\n",
    "            \"images\": pixel_values,   # [3, 336, 336] tensor on cpu\n",
    "            \"question\": question,           # Could you confirm if this is <dunpai> in the photo?\n",
    "            \"answer\": answer,               # I can confirm that this is not <dunpai> in the photo.\n",
    "            \"prompt_w_answer\": prompt_w_answer, #  USER: Could you confirm if this is <dunpai> in the photo? ASSISTANT: I can confirm that this is not <dunpai> in the photo.<|endoftext|>\n",
    "            \"prompt_w_o_answer\": prompt_w_o_answer  #  USER: Could you confirm if this is <dunpai> in the photo?\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =   PersonalizedMMUDataset('/home/arc/full_mcdata', \n",
    "                                         'dunpai', \n",
    "                                         clip_image_processor=clip_image_processor,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[ 0.9668,  0.9668,  0.9668,  ..., -0.1426, -0.1572, -0.1572],\n",
       "          [ 0.9668,  0.9668,  0.9668,  ..., -0.1426, -0.1572, -0.1572],\n",
       "          [ 0.9814,  0.9814,  0.9814,  ..., -0.1280, -0.1426, -0.1426],\n",
       "          ...,\n",
       "          [-1.2229, -1.2229, -1.2229,  ..., -1.4711, -1.4565, -1.4419],\n",
       "          [-1.2229, -1.2229, -1.2229,  ..., -1.4419, -1.4273, -1.4127],\n",
       "          [-1.2229, -1.2229, -1.2083,  ..., -1.4419, -1.4273, -1.4127]],\n",
       " \n",
       "         [[ 1.0544,  1.0544,  1.0544,  ...,  0.0638,  0.0488,  0.0488],\n",
       "          [ 1.0544,  1.0544,  1.0544,  ...,  0.0638,  0.0488,  0.0488],\n",
       "          [ 1.0694,  1.0694,  1.0694,  ...,  0.0789,  0.0638,  0.0638],\n",
       "          ...,\n",
       "          [-1.2718, -1.2718, -1.2718,  ..., -1.4369, -1.4219, -1.4069],\n",
       "          [-1.2718, -1.2718, -1.2718,  ..., -1.4069, -1.3919, -1.3769],\n",
       "          [-1.2718, -1.2718, -1.2568,  ..., -1.4069, -1.3919, -1.3769]],\n",
       " \n",
       "         [[ 1.0083,  1.0083,  1.0083,  ...,  0.2688,  0.2546,  0.2546],\n",
       "          [ 1.0083,  1.0083,  1.0083,  ...,  0.2688,  0.2546,  0.2546],\n",
       "          [ 1.0225,  1.0225,  1.0225,  ...,  0.2831,  0.2688,  0.2688],\n",
       "          ...,\n",
       "          [-1.1389, -1.1105, -1.1105,  ..., -1.2100, -1.1958, -1.1816],\n",
       "          [-1.1389, -1.1247, -1.1105,  ..., -1.1816, -1.1674, -1.1532],\n",
       "          [-1.1389, -1.1247, -1.0963,  ..., -1.1816, -1.1674, -1.1532]]]),\n",
       " 'question': 'Could you confirm if this is <dunpai> in the photo?',\n",
       " 'answer': 'I can confirm that this is not <dunpai> in the photo.',\n",
       " 'prompt_w_answer': ' USER: Could you confirm if this is <dunpai> in the photo? ASSISTANT: I can confirm that this is not <dunpai> in the photo.<|endoftext|>',\n",
       " 'prompt_w_o_answer': ' USER: Could you confirm if this is <dunpai> in the photo? '}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def image_transform(sample, resolution=256):\n",
    "    # input image is PIL image\n",
    "    image = sample[\"images\"]\n",
    "    image = transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BICUBIC)(image)\n",
    "    image = transforms.CenterCrop((resolution, resolution))(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)(image)\n",
    "    sample[\"images\"] = image\n",
    "    return sample\n",
    "\n",
    "data_root = '/home/arc/full_mcdata'\n",
    "data_w_cases_path = os.path.join(data_root, \"test_concepts.json\")\n",
    "with open(data_w_cases_path, \"r\") as f:\n",
    "    data_w_cases = json.load(f)\n",
    "\n",
    "class PersonalizedT2IDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        concept_name: str,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.concept_name = concept_name\n",
    "        \n",
    "        case_type = data_w_cases[concept_name]\n",
    "        training_img_dir_path = os.path.join(data_root, case_type, \"concept/train\", concept_name)\n",
    "        self.img_paths = []\n",
    "        for img in os.listdir(training_img_dir_path):\n",
    "            if img.endswith(('png', 'jpg', 'jpeg')) and \"mask\" not in img:\n",
    "                img_path = os.path.join(training_img_dir_path, img)\n",
    "                self.img_paths.append(img_path)\n",
    "                \n",
    "        assert len(self.img_paths) == 10, f\"Expected 10 images for mcllava dataset, found {len(img_paths)}\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        item = {\n",
    "            \"condition\": f\"A photo of <{self.concept_name}>.\",\n",
    "            \"images\": img,  # [3, 256, 256] tensor on cpu\n",
    "        }\n",
    "        item = image_transform(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2i_loader = DataLoader(\n",
    "    PersonalizedT2IDataset(data_root, \"dunpai\"),\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in t2i_loader:\n",
    "    b = batch\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2i_dataset = PersonalizedT2IDataset(data_root, \"dunpai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2i_dataset[0][\"images\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdata import get_personalized_mmudataloader\n",
    "\n",
    "mmu_loader = get_personalized_mmudataloader('/home/arc/full_mcdata', \"dunpai\", tokenizer, batch_size=5, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mmu_dataset = list(mmu_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = list_mmu_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n",
       "        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n",
       "        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n",
       "        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n",
       "        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n",
       "        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n",
       "        10705,  8808,  8643,    25,  1279,    67,   403, 49712,    29,   468,\n",
       "          257,  7209,   290, 40551, 11743,    13, 50256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
