{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-03 21:10:47,840] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from pdata import PersonalizedMMUDataset, PersonalizedT2IDataset, get_personalized_mmudataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from models import Showo, MAGVITv2, get_mask_chedule\n",
    "from training.prompting_utils import UniversalPrompting, create_attention_mask_for_mmu, create_attention_mask_for_mmu_vit\n",
    "from training.utils import get_config, flatten_omega_conf, mask_or_random_replace_tokens, AverageMeter\n",
    "from transformers import AutoTokenizer\n",
    "from models.clip_encoder import CLIPVisionTower\n",
    "from transformers import CLIPImageProcessor\n",
    "from llava.llava import conversation as conversation_lib\n",
    "\n",
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"phi1.5\"]\n",
    "\n",
    "import os\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "config = OmegaConf.load('configs/showo_demo.yaml')\n",
    "# device setup\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 13, 16, 16) = 3328 dimensions.\n",
      "Look-up free quantizer with codebook size: 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'mask_token_id': 58497} were passed to Showo, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/home/arc/Show-o/models/modeling_showo.py:49: FutureWarning: Accessing config attribute `w_clip_vit` directly via 'Showo' object attribute is deprecated. Please access 'w_clip_vit' over 'Showo's config object instead, e.g. 'unet.config.w_clip_vit'.\n",
      "  if self.w_clip_vit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention implementation:  sdpa\n"
     ]
    }
   ],
   "source": [
    "# config load -  'showo_demo_w_clip_vit.yaml'\n",
    "\n",
    "# show o tokenizer setup and adding special tokens to universal prompting\n",
    "# llm model : 'microsoft/phi-1_5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.showo.llm_model_path, padding_side =\"left\")\n",
    "uni_prompting = UniversalPrompting(tokenizer, max_text_len=config.dataset.preprocessing.max_seq_length,\n",
    "                                       special_tokens=(\"<|soi|>\", \"<|eoi|>\", \"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n",
    "                                       ignore_id=-100, cond_dropout_prob=config.training.cond_dropout_prob)\n",
    "\n",
    "# setting up the magvit-v2, for t2i\n",
    "vq_model = MAGVITv2\n",
    "vq_model = vq_model.from_pretrained(config.model.vq_model.vq_model_name).to(device)\n",
    "# vq_model.requires_grad_(False)\n",
    "# vq_model.eval()\n",
    "\n",
    "# setting up vision tower: clip-vit only for mmu\n",
    "# vision_tower_name =config.clip_path\n",
    "# vision_tower = CLIPVisionTower(vision_tower_name).to(device)\n",
    "# clip_image_processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n",
    "\n",
    "# setting up the showo model \n",
    "model = Showo.from_pretrained(config.model.showo.pretrained_model_path).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# setting up the parameters\n",
    "temperature = 0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 1  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "LLAVA_SYSTEM_PROMPT = \"A chat between a curious user and an artificial intelligence assistant. \" \\\n",
    "                \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "LLAVA_SYSTEM_PROMPT_LEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/arc/full_mcdata\"\n",
    "concept = \"dunpai\"\n",
    "\n",
    "new_tokens = [f\"<{concept}>\"] + [f\"<token_{i}>\" for i in range(16)]\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "model.showo.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "placeholder_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(token)\n",
    "    for token in new_tokens\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3562240/3583499484.py:2: FutureWarning: Accessing config attribute `mask_token_id` directly via 'Showo' object attribute is deprecated. Please access 'mask_token_id' over 'Showo's config object instead, e.g. 'unet.config.mask_token_id'.\n",
      "  mask_id = model.mask_token_id\n"
     ]
    }
   ],
   "source": [
    "mask_schedule = get_mask_chedule(config.training.get(\"mask_schedule\", \"cosine\"))\n",
    "mask_id = model.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t2i_dataset = PersonalizedT2IDataset(data_root, concept)\n",
    "t2i_dataloader = DataLoader(t2i_dataset, batch_size=5, shuffle=True, num_workers=10, pin_memory=True)\n",
    "\n",
    "mmu_dataloader = get_personalized_mmudataloader(data_root, concept, tokenizer, batch_size=5, num_workers=10)\n",
    "\n",
    "iterables = {\n",
    "    'mmu_flow': mmu_dataloader,\n",
    "    't2i_flow': t2i_dataloader\n",
    "}\n",
    "\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "\n",
    "combined_dataloader = CombinedLoader(iterables, mode=\"max_size_cycle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size = 58498 = 50295  llm vocabsize\n",
    "#                    + 10     <|soi|> <|eoi|> <|sov|> <|eov|> <|t2i|> <|mmu|> <|t2v|> <|v2v|> <|lvg|> <|pad|>\n",
    "#                    + 8192   vq model codebook size\n",
    "#                    + 1      mask token (token id == 58497)\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "uni_prompting.sptids_dict\n",
    "# {'<|soi|>': tensor([50296]),\n",
    "#  '<|eoi|>': tensor([50297]),\n",
    "#  '<|sov|>': tensor([50298]),\n",
    "#  '<|eov|>': tensor([50299]),\n",
    "#  '<|t2i|>': tensor([50300]),\n",
    "#  '<|mmu|>': tensor([50301]),\n",
    "#  '<|t2v|>': tensor([50302]),\n",
    "#  '<|v2v|>': tensor([50303]),\n",
    "#  '<|lvg|>': tensor([50304]),\n",
    "#  '<|sot|>': tensor([50256]),\n",
    "#  '<|eot|>': tensor([50256]),\n",
    "#  '<|pad|>': tensor([50295])}\n",
    "\n",
    "# uni_prompting.text_tokenizer == tokenizer\n",
    "def prepare_inputs_and_labels(\n",
    "        pixel_values_or_image_ids: Union[torch.FloatTensor, torch.LongTensor],\n",
    "        texts: Union[str, str],\n",
    "        min_masking_rate: float = 0.0,\n",
    "        is_train: bool = True,\n",
    "):\n",
    "\n",
    "    image_tokens = vq_model.get_code(pixel_values_or_image_ids)\n",
    "    image_tokens = image_tokens + len(uni_prompting.text_tokenizer)\n",
    "\n",
    "    # create MLM mask and labels\n",
    "    input_ids, labels, loss_weight, mask_prob = mask_or_random_replace_tokens(\n",
    "        image_tokens,\n",
    "        mask_id,\n",
    "        config,\n",
    "        mask_schedule=mask_schedule,\n",
    "        is_train=is_train,\n",
    "    )\n",
    "    input_ids, masks, labels = uni_prompting((texts, input_ids, labels), 't2i')\n",
    "\n",
    "    return input_ids, labels, mask_prob, image_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<dunpai>',\n",
       " '<token_0>',\n",
       " '<token_1>',\n",
       " '<token_2>',\n",
       " '<token_3>',\n",
       " '<token_4>',\n",
       " '<token_5>',\n",
       " '<token_6>',\n",
       " '<token_7>',\n",
       " '<token_8>',\n",
       " '<token_9>',\n",
       " '<token_10>',\n",
       " '<token_11>',\n",
       " '<token_12>',\n",
       " '<token_13>',\n",
       " '<token_14>',\n",
       " '<token_15>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = list(combined_dataloader)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "one_batch_t2i = list_data[0][0]['t2i_flow']\n",
    "one_batch_mmu = list_data[0][0]['mmu_flow']\n",
    "one_batch_size_t2i = one_batch_t2i[\"images\"].shape[0]\n",
    "one_batch_size_mmu = one_batch_mmu[\"images\"].shape[0]\n",
    "print(one_batch_size_t2i, one_batch_size_mmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2i flow\n",
    "pixel_values, texts = one_batch_t2i[\"images\"], one_batch_t2i[\"conditions\"]\n",
    "texts_ids = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# input_ids, labels, mask_prob, image_tokens_ori = prepare_inputs_and_labels(pixel_values, texts_ids, is_train=True)\n",
    "pixel_values = pixel_values.to(device)\n",
    "texts_ids = texts_ids.to(device)\n",
    "input_ids, labels, mask_prob, image_tokens_ori = prepare_inputs_and_labels(pixel_values, texts, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape # [5, 387] = [batch_size, ~ 128 + 256 = 384]\n",
    "labels.shape # [5, 387]\n",
    "mask_prob.shape # [5]\n",
    "image_tokens_ori.shape # [5, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.prompting_utils import create_attention_mask_predict_next, create_attention_mask_for_mmu\n",
    "attention_mask = create_attention_mask_predict_next(input_ids,\n",
    "                                                    pad_id=int(uni_prompting.sptids_dict['<|pad|>']),\n",
    "                                                    soi_id=int(uni_prompting.sptids_dict['<|soi|>']),\n",
    "                                                    eoi_id=int(uni_prompting.sptids_dict['<|eoi|>']),\n",
    "                                                    rm_pad_in_image=True,\n",
    "                                                    return_inverse_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values_mmu, input_ids_mmu, labels_mmu = (one_batch_mmu[\"images\"],\n",
    "                                               one_batch_mmu[\"input_ids\"],\n",
    "                                               one_batch_mmu[\"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting llava instruction data\n"
     ]
    }
   ],
   "source": [
    "from llava.llava_data_vq_unified import get_instruct_data_loader_single_gpu\n",
    "\n",
    "mmu_dataloader_new = get_instruct_data_loader_single_gpu(\n",
    "                                    tokenizer,\n",
    "                                    batch_size=5,\n",
    "                                    num_workers=10,\n",
    "                                    max_length=128,\n",
    "                                    phase=\"tuning\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "list_data_mmu = list(mmu_dataloader_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch = list_data_mmu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch[\"labels\"].shape\n",
    "a_batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values_mmu, input_ids_mmu, labels_mmu = (a_batch[\"images\"],\n",
    "                                               a_batch[\"input_ids\"],\n",
    "                                               a_batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values_mmu = pixel_values_mmu.to(device, non_blocking=True)\n",
    "input_ids_mmu = input_ids_mmu.to(device, non_blocking=True)\n",
    "image_tokens_mmu = vq_model.get_code(pixel_values_mmu)\n",
    "image_tokens_mmu = image_tokens_mmu + len(uni_prompting.text_tokenizer)\n",
    "\n",
    "input_ids_mmu = torch.cat([\n",
    "    (torch.ones(input_ids_mmu.shape[0], 1) * uni_prompting.sptids_dict['<|mmu|>']).to(\n",
    "        device),\n",
    "    (torch.ones(input_ids_mmu.shape[0], 1) * uni_prompting.sptids_dict['<|soi|>']).to(\n",
    "        device),\n",
    "    image_tokens_mmu,\n",
    "    (torch.ones(input_ids_mmu.shape[0], 1) * uni_prompting.sptids_dict['<|eoi|>']).to(\n",
    "        device),\n",
    "    input_ids_mmu,\n",
    "], dim=1).long()\n",
    "\n",
    "labels_mmu = torch.cat([\n",
    "    (torch.ones(input_ids_mmu.shape[0], 1) * uni_prompting.ignore_id).to(device),\n",
    "    (torch.ones(input_ids_mmu.shape[0], 1) * uni_prompting.ignore_id).to(device),\n",
    "    torch.ones_like(image_tokens_mmu) * uni_prompting.ignore_id,\n",
    "    (torch.ones(input_ids_mmu.shape[0], 1) * uni_prompting.ignore_id).to(device),\n",
    "    labels_mmu.to(device)\n",
    "], dim=1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50301, 50296, 50424,  ..., 50295, 50295, 50295],\n",
       "        [50301, 50296, 56637,  ..., 50295, 50295, 50295],\n",
       "        [50301, 50296, 54859,  ..., 50295, 50295, 50295],\n",
       "        [50301, 50296, 52541,  ..., 50295, 50295, 50295],\n",
       "        [50301, 50296, 52541,  ..., 50295, 50295, 50295]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode labels_mmu, ignore id = -100\n",
    "decode_mmu_labels = labels_mmu[0]\n",
    "decode_mmu_labels = decode_mmu_labels[decode_mmu_labels != -100]\n",
    "\n",
    "# tokenizer.decode(decode_mmu_labels)\n",
    "input_ids_mmu[0]\n",
    "\n",
    "# input_ids_mmu[0].shape\n",
    "input_ids_mmu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
