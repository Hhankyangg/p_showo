{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/miniconda3/envs/showo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-01 01:45:58,353] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/miniconda3/envs/showo/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import DistributedType, set_seed\n",
    "\n",
    "from training.data import Text2ImageDataset\n",
    "from training.imagenet_dataset import ImageNetDataset\n",
    "from models import Showo, MAGVITv2, get_mask_chedule\n",
    "from training.prompting_utils import UniversalPrompting, create_attention_mask_predict_next, \\\n",
    "    create_attention_mask_for_mmu\n",
    "from models.lr_schedulers import get_scheduler\n",
    "from models.logging import set_verbosity_info, set_verbosity_error\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from llava.llava_data_vq_unified import get_instruct_data_loader\n",
    "\n",
    "SYSTEM_PROMPT_LEN = 28\n",
    "\n",
    "from training.utils import get_config, flatten_omega_conf, mask_or_random_replace_tokens, AverageMeter\n",
    "\n",
    "try:\n",
    "    import apex\n",
    "\n",
    "    is_apex_available = True\n",
    "except ImportError:\n",
    "    is_apex_available = False\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "config = OmegaConf.load('configs/showo_demo.yaml')\n",
    "# device setup\n",
    "device = torch.device(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 13, 16, 16) = 3328 dimensions.\n",
      "Look-up free quantizer with codebook size: 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'mask_token_id': 58497} were passed to Showo, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/home/arc/Show-o/models/modeling_showo.py:49: FutureWarning: Accessing config attribute `w_clip_vit` directly via 'Showo' object attribute is deprecated. Please access 'w_clip_vit' over 'Showo's config object instead, e.g. 'unet.config.w_clip_vit'.\n",
      "  if self.w_clip_vit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention implementation:  sdpa\n"
     ]
    }
   ],
   "source": [
    "# config load -  'showo_demo_w_clip_vit.yaml'\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "# show o tokenizer setup and adding special tokens to universal prompting\n",
    "# llm model : 'microsoft/phi-1_5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.showo.llm_model_path, padding_side =\"left\")\n",
    "uni_prompting = UniversalPrompting(tokenizer, max_text_len=config.dataset.preprocessing.max_seq_length,\n",
    "                                       special_tokens=(\"<|soi|>\", \"<|eoi|>\", \"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n",
    "                                       ignore_id=-100, cond_dropout_prob=config.training.cond_dropout_prob)\n",
    "\n",
    "# setting up the visual question answering model: magvit-v2\n",
    "vq_model = MAGVITv2\n",
    "vq_model = vq_model.from_pretrained(config.model.vq_model.vq_model_name).to(device)\n",
    "vq_model.requires_grad_(False)\n",
    "vq_model.eval()\n",
    "\n",
    "# setting up vision tower: clip-vit\n",
    "# vision_tower_name =config.clip_path\n",
    "# vision_tower = CLIPVisionTower(vision_tower_name).to(device)\n",
    "# clip_image_processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n",
    "\n",
    "# setting up the showo model \n",
    "model = Showo.from_pretrained(config.model.showo.pretrained_model_path).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# setting up the parameters\n",
    "temperature = 0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 1  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "SYSTEM_PROMPT = \"A chat between a curious user and an artificial intelligence assistant. \" \\\n",
    "                \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "SYSTEM_PROMPT_LEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_and_labels(\n",
    "        pixel_values_or_image_ids: Union[torch.FloatTensor, torch.LongTensor],\n",
    "        texts: Union[str, str],\n",
    "        min_masking_rate: float = 0.0,\n",
    "        is_train: bool = True,\n",
    "):\n",
    "\n",
    "    image_tokens = vq_model.get_code(pixel_values_or_image_ids)\n",
    "    image_tokens = image_tokens + len(uni_prompting.text_tokenizer)\n",
    "\n",
    "    # create MLM mask and labels\n",
    "    input_ids, labels, loss_weight, mask_prob = mask_or_random_replace_tokens(\n",
    "        image_tokens,\n",
    "        mask_id,\n",
    "        config,\n",
    "        mask_schedule=mask_schedule,\n",
    "        is_train=is_train,\n",
    "    )\n",
    "    input_ids, masks, labels = uni_prompting((texts, input_ids, labels), 't2i')\n",
    "\n",
    "    return input_ids, labels, mask_prob, image_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50305"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_prompting.text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
